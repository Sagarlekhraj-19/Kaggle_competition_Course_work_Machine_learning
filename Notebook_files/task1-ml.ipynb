{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116866,"databundleVersionId":13944237,"sourceType":"competition"},{"sourceId":13510991,"sourceType":"datasetVersion","datasetId":8578386}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport ydata_profiling as pp\ndf=pd.read_csv(\"/kaggle/input/enhanced-safe-driver-prediction-challenge/train1.csv\")\ndf.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T08:07:00.446476Z","iopub.execute_input":"2025-10-31T08:07:00.446796Z","iopub.status.idle":"2025-10-31T08:07:02.432015Z","shell.execute_reply.started":"2025-10-31T08:07:00.446772Z","shell.execute_reply":"2025-10-31T08:07:02.429063Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"        id  ps_ind_02_cat  ps_ind_04_cat  ps_ind_05_cat  ps_car_01_cat  \\\n0  1158448            1.0            1.0            0.0            7.0   \n1   341018            2.0            1.0            0.0            7.0   \n2   699143            2.0            0.0            0.0            4.0   \n3   744070            1.0            1.0            0.0            3.0   \n4   639390            2.0            0.0            0.0           11.0   \n\n   ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  \\\n0            1.0            1.0              0            1.0             10   \n1            1.0            NaN              0            NaN             11   \n2            1.0            NaN              0            1.0              1   \n3            1.0            NaN              2            NaN              1   \n4            1.0            NaN              2            NaN             11   \n\n   ...  ps_calc_20_bin  feature1  feature2  feature3  feature4  feature5  \\\n0  ...               0         0  4.136926         3  0.777766        25   \n1  ...               0         0  0.592341         3  0.770527         1   \n2  ...               0         0  0.950207         6  0.757445         4   \n3  ...               1         0  2.013771         5       NaN         4   \n4  ...               0         0  0.000000         3  0.767450         0   \n\n        feature6  feature7  feature8  target  \n0       0.165477  3.010493        16       0  \n1       0.592340  3.036803        16       0  \n2       0.237552  2.536030        16       0  \n3       0.503442  3.251724        15       0  \n4  793360.881100  2.703617        18       1  \n\n[5 rows x 67 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_car_01_cat</th>\n      <th>ps_car_02_cat</th>\n      <th>ps_car_03_cat</th>\n      <th>ps_car_04_cat</th>\n      <th>ps_car_05_cat</th>\n      <th>ps_car_06_cat</th>\n      <th>...</th>\n      <th>ps_calc_20_bin</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature4</th>\n      <th>feature5</th>\n      <th>feature6</th>\n      <th>feature7</th>\n      <th>feature8</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1158448</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.136926</td>\n      <td>3</td>\n      <td>0.777766</td>\n      <td>25</td>\n      <td>0.165477</td>\n      <td>3.010493</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>341018</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.592341</td>\n      <td>3</td>\n      <td>0.770527</td>\n      <td>1</td>\n      <td>0.592340</td>\n      <td>3.036803</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>699143</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.950207</td>\n      <td>6</td>\n      <td>0.757445</td>\n      <td>4</td>\n      <td>0.237552</td>\n      <td>2.536030</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>744070</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.013771</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0.503442</td>\n      <td>3.251724</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>639390</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>3</td>\n      <td>0.767450</td>\n      <td>0</td>\n      <td>793360.881100</td>\n      <td>2.703617</td>\n      <td>18</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 67 columns</p>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T08:07:13.701593Z","iopub.execute_input":"2025-10-31T08:07:13.701911Z","iopub.status.idle":"2025-10-31T08:07:13.708744Z","shell.execute_reply.started":"2025-10-31T08:07:13.701889Z","shell.execute_reply":"2025-10-31T08:07:13.707668Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(296209, 67)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T08:07:16.492951Z","iopub.execute_input":"2025-10-31T08:07:16.493644Z","iopub.status.idle":"2025-10-31T08:07:16.571641Z","shell.execute_reply.started":"2025-10-31T08:07:16.493610Z","shell.execute_reply":"2025-10-31T08:07:16.570737Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 296209 entries, 0 to 296208\nData columns (total 67 columns):\n #   Column          Non-Null Count   Dtype  \n---  ------          --------------   -----  \n 0   id              296209 non-null  int64  \n 1   ps_ind_02_cat   296084 non-null  float64\n 2   ps_ind_04_cat   296164 non-null  float64\n 3   ps_ind_05_cat   293294 non-null  float64\n 4   ps_car_01_cat   296152 non-null  float64\n 5   ps_car_02_cat   296206 non-null  float64\n 6   ps_car_03_cat   91620 non-null   float64\n 7   ps_car_04_cat   296209 non-null  int64  \n 8   ps_car_05_cat   163922 non-null  float64\n 9   ps_car_06_cat   296209 non-null  int64  \n 10  ps_car_07_cat   290426 non-null  float64\n 11  ps_car_08_cat   296209 non-null  int64  \n 12  ps_car_09_cat   295921 non-null  float64\n 13  ps_car_10_cat   296209 non-null  int64  \n 14  ps_car_11_cat   296209 non-null  int64  \n 15  ps_ind_01       296209 non-null  int64  \n 16  ps_ind_03       296209 non-null  int64  \n 17  ps_ind_06_bin   296209 non-null  int64  \n 18  ps_ind_07_bin   296209 non-null  int64  \n 19  ps_ind_08_bin   296209 non-null  int64  \n 20  ps_ind_09_bin   296209 non-null  int64  \n 21  ps_ind_10_bin   296209 non-null  int64  \n 22  ps_ind_11_bin   296209 non-null  int64  \n 23  ps_ind_12_bin   296209 non-null  int64  \n 24  ps_ind_13_bin   296209 non-null  int64  \n 25  ps_ind_14       296209 non-null  int64  \n 26  ps_ind_15       296209 non-null  int64  \n 27  ps_ind_16_bin   296209 non-null  int64  \n 28  ps_ind_17_bin   296209 non-null  int64  \n 29  ps_ind_18_bin   296209 non-null  int64  \n 30  ps_reg_01       296209 non-null  float64\n 31  ps_reg_02       296209 non-null  float64\n 32  ps_reg_03       242630 non-null  float64\n 33  ps_car_11       296205 non-null  float64\n 34  ps_car_12       296208 non-null  float64\n 35  ps_car_13       296209 non-null  float64\n 36  ps_car_14       275101 non-null  float64\n 37  ps_car_15       296209 non-null  float64\n 38  ps_calc_01      296209 non-null  float64\n 39  ps_calc_02      296209 non-null  float64\n 40  ps_calc_03      296209 non-null  float64\n 41  ps_calc_04      296209 non-null  int64  \n 42  ps_calc_05      296209 non-null  int64  \n 43  ps_calc_06      296209 non-null  int64  \n 44  ps_calc_07      296209 non-null  int64  \n 45  ps_calc_08      296209 non-null  int64  \n 46  ps_calc_09      296209 non-null  int64  \n 47  ps_calc_10      296209 non-null  int64  \n 48  ps_calc_11      296209 non-null  int64  \n 49  ps_calc_12      296209 non-null  int64  \n 50  ps_calc_13      296209 non-null  int64  \n 51  ps_calc_14      296209 non-null  int64  \n 52  ps_calc_15_bin  296209 non-null  int64  \n 53  ps_calc_16_bin  296209 non-null  int64  \n 54  ps_calc_17_bin  296209 non-null  int64  \n 55  ps_calc_18_bin  296209 non-null  int64  \n 56  ps_calc_19_bin  296209 non-null  int64  \n 57  ps_calc_20_bin  296209 non-null  int64  \n 58  feature1        296209 non-null  int64  \n 59  feature2        296209 non-null  float64\n 60  feature3        296209 non-null  int64  \n 61  feature4        242630 non-null  float64\n 62  feature5        296209 non-null  int64  \n 63  feature6        296209 non-null  float64\n 64  feature7        296209 non-null  float64\n 65  feature8        296209 non-null  int64  \n 66  target          296209 non-null  int64  \ndtypes: float64(24), int64(43)\nmemory usage: 151.4 MB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"profile=pp.ProfileReport(df)\nprofile.to_file(\"file_report.pdf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T08:19:13.041625Z","iopub.execute_input":"2025-10-31T08:19:13.041980Z","iopub.status.idle":"2025-10-31T08:26:02.383389Z","shell.execute_reply.started":"2025-10-31T08:19:13.041940Z","shell.execute_reply":"2025-10-31T08:26:02.382142Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b34cf139fac47c9be721b9e26f28edf"}},"metadata":{}},{"name":"stderr","text":"\n  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n  1%|â–         | 1/67 [00:02<02:37,  2.39s/it]\u001b[A\n 12%|â–ˆâ–        | 8/67 [00:02<00:16,  3.60it/s]\u001b[A\n 13%|â–ˆâ–Ž        | 9/67 [00:04<00:23,  2.47it/s]\u001b[A\n 15%|â–ˆâ–        | 10/67 [00:04<00:20,  2.78it/s]\u001b[A\n 16%|â–ˆâ–‹        | 11/67 [00:04<00:17,  3.13it/s]\u001b[A\n 18%|â–ˆâ–Š        | 12/67 [00:04<00:15,  3.49it/s]\u001b[A\n 19%|â–ˆâ–‰        | 13/67 [00:04<00:13,  3.88it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–       | 15/67 [00:05<00:12,  4.15it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–       | 16/67 [00:05<00:12,  4.24it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:05<00:16,  3.08it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–‹       | 18/67 [00:06<00:14,  3.41it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–Š       | 19/67 [00:06<00:16,  2.94it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–‰       | 20/67 [00:06<00:13,  3.58it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:06<00:12,  3.80it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:07<00:06,  6.64it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/67 [00:07<00:04,  8.69it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:08<00:04,  7.40it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:08<00:03,  9.14it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:08<00:02, 12.86it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:08<00:01, 16.85it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:08<00:00, 19.05it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 52/67 [00:10<00:02,  6.21it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:10<00:00,  8.75it/s]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:11<00:00,  6.00it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceeb8917617f436184729ffcace512a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7779d798b34a61924ee57d6702f5b1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/ydata_profiling/profile_report.py:386: UserWarning: Extension .pdf not supported. For now we assume .html was intended. To remove this warning, please use .html or .json.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ad92a70d97452f84cc6fddd654142d"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"\ncols_to_drop = ['id', 'feature1', 'feature7']\ndf = df.drop(columns=cols_to_drop, errors='ignore')\n\nprint(\"Dropped columns:\", cols_to_drop)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_missing = ['ps_car_03_cat', 'ps_car_05_cat']\ndf = df.drop(columns=high_missing, errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in df.columns:\n    if df[col].dtype == 'object':\n        df[col].fillna(df[col].mode()[0], inplace=True)\n    else:\n        df[col].fillna(df[col].median(), inplace=True)\n\nprint(\"Handled missing values âœ…\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Identify categorical columns (assuming object or categorical dtype)\ncategorical_cols = df.select_dtypes(include=['object', 'category']).columns\n\n# Encode\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])\n\nprint(\"Encoded categorical columns:\", list(categorical_cols))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate target\nX = df.drop(columns=['target'])\ny = df['target']\n\n# Split (70/30)\n\nERP_ID=29325\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Test shape:\", X_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n# Initialize Decision Tree\ndt = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',  # handles target imbalance\n    max_depth=6,     # try smaller depth\n    min_samples_leaf=20\n)\n\n# Train\ndt.fit(X_train, y_train)\n\n# Evaluate\ny_pred = dt.predict(X_test)\ny_pred_proba = dt.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Get false positive rate, true positive rate, and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\n# Calculate AUC\nroc_auc = auc(fpr, tpr)\n\n# Plot\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Guess')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\nplt.legend(loc=\"lower right\")\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nimport numpy as np\nimport pandas as pd\n\n# --- Separate numeric and categorical columns ---\nnum_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n\n# --- Encode categorical columns (if not numeric yet) ---\nif len(cat_cols) > 0:\n    X_train_encoded = X_train.copy()\n    for col in cat_cols:\n        le = LabelEncoder()\n        X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))\nelse:\n    X_train_encoded = X_train.copy()\n\n# --- 1ï¸âƒ£ ANOVA for numeric ---\nanova_selector = SelectKBest(score_func=f_classif, k=min(20, len(num_cols)))\nanova_selector.fit(X_train_encoded[num_cols], y_train)\nanova_features = [num_cols[i] for i in anova_selector.get_support(indices=True)]\n\n# --- 2ï¸âƒ£ Chi-Square for categorical (scaled 0-1) ---\nchi2_features = []\nif len(cat_cols) > 0:\n    scaler = MinMaxScaler()\n    X_cat_scaled = scaler.fit_transform(X_train_encoded[cat_cols])\n    chi2_selector = SelectKBest(score_func=chi2, k=min(10, len(cat_cols)))\n    chi2_selector.fit(X_cat_scaled, y_train)\n    chi2_features = [cat_cols[i] for i in chi2_selector.get_support(indices=True)]\n\n# --- Combine ---\nselected_features = list(set(anova_features + chi2_features))\nprint(\"âœ… Selected Features (ANOVA + Chi2):\")\nprint(selected_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Filter the training and testing data using selected features ---\nX_train_selected = X_train[selected_features]\nX_test_selected = X_test[selected_features]\n\n# --- Initialize Decision Tree (balanced for imbalanced target) ---\ndt = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',   # handles target imbalance\n    max_depth=6,               # moderate depth to avoid overfitting\n    min_samples_leaf=20        # ensures each leaf has enough samples\n)\n\n# --- Train ---\ndt.fit(X_train_selected, y_train)\n\n# --- Evaluate ---\ny_pred = dt.predict(X_test_selected)\ny_pred_proba = dt.predict_proba(X_test_selected)[:, 1]\n\nprint(\"\\n=== Decision Tree (ANOVA + ChiÂ² Features) ===\")\nprint(classification_report(y_test, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Use your already selected features\nX_train_sel = X_train[selected_features]\n\n# Initialize Decision Tree (use same settings as your baseline)\ndt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n\n# --- 1ï¸âƒ£ Forward Selection ---\nsfs_forward = SequentialFeatureSelector(\n    estimator=dt,\n    n_features_to_select=15,     # you can change to a smaller/larger number\n    direction='forward',\n    scoring='roc_auc',\n    cv=3,\n    n_jobs=-1\n)\n\nsfs_forward.fit(X_train_sel, y_train)\nforward_features = X_train_sel.columns[sfs_forward.get_support()].tolist()\n\nprint(\"âœ… Selected Features (Forward Selection):\")\nprint(forward_features)\n\n\n# --- 2ï¸âƒ£ Backward Elimination ---\nsfs_backward = SequentialFeatureSelector(\n    estimator=dt,\n    n_features_to_select=15,     # same number for fair comparison\n    direction='backward',\n    scoring='roc_auc',\n    cv=3,\n    n_jobs=-1\n)\n\nsfs_backward.fit(X_train_sel, y_train)\nbackward_features = X_train_sel.columns[sfs_backward.get_support()].tolist()\n\nprint(\"\\nâœ… Selected Features (Backward Elimination):\")\nprint(backward_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Forward Selection Evaluation ---\nX_train_forward = X_train[forward_features]\nX_test_forward = X_test[forward_features]\n\ndt_forward = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',\n    max_depth=6,\n    min_samples_leaf=20\n)\n\ndt_forward.fit(X_train_forward, y_train)\n\ny_pred_forward = dt_forward.predict(X_test_forward)\ny_pred_proba_forward = dt_forward.predict_proba(X_test_forward)[:, 1]\n\nprint(\"\\n=== Decision Tree (Forward Selection) ===\")\nprint(classification_report(y_test, y_pred_forward))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba_forward))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Backward Elimination Evaluation ---\nX_train_backward = X_train[backward_features]\nX_test_backward = X_test[backward_features]\n\ndt_backward = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',\n    max_depth=6,\n    min_samples_leaf=20\n)\n\ndt_backward.fit(X_train_backward, y_train)\n\ny_pred_backward = dt_backward.predict(X_test_backward)\ny_pred_proba_backward = dt_backward.predict_proba(X_test_backward)[:, 1]\n\nprint(\"\\n=== Decision Tree (Backward Elimination) ===\")\nprint(classification_report(y_test, y_pred_backward))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba_backward))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Step 1: Select features (using forward_features from your notebook) ---\n# Replace 'forward_features' with your selected feature list\nforward_features = ['feature4', 'ps_car_08_cat', 'ps_ind_15', 'ps_ind_16_bin', \n                    'ps_ind_05_cat', 'ps_ind_07_bin', 'ps_car_12', 'ps_car_04_cat', \n                    'ps_car_02_cat', 'ps_car_13', 'ps_car_07_cat', 'ps_ind_17_bin', \n                    'ps_reg_01', 'ps_car_01_cat', 'ps_ind_06_bin']\n\nX_train_sel = X_train[forward_features]\nX_test_sel = X_test[forward_features]\n\n# --- Step 2: Standardize the features ---\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sel)\nX_test_scaled = scaler.transform(X_test_sel)\n\n# --- Step 3: Fit PCA to inspect explained variance ---\npca = PCA().fit(X_train_scaled)\ncum_var = np.cumsum(pca.explained_variance_ratio_)\n\n# --- Step 4: Find number of components for 90% variance ---\nn_components_90 = np.argmax(cum_var >= 0.90) + 1\nprint(f\"âœ… Number of components capturing 90% variance: {n_components_90}\")\n\n# --- Step 5: Plot cumulative variance ---\nplt.figure(figsize=(8, 5))\nplt.plot(cum_var, marker='o')\nplt.axhline(y=0.90, color='r', linestyle='--', label='90% variance threshold')\nplt.axvline(x=n_components_90-1, color='g', linestyle='--', \n            label=f'{n_components_90} components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA â€” Cumulative Explained Variance')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# --- Step 6: Apply PCA with optimal number of components ---\npca_reduced = PCA(n_components=n_components_90, random_state=29325)\nX_train_pca = pca_reduced.fit_transform(X_train_scaled)\nX_test_pca = pca_reduced.transform(X_test_scaled)\n\nprint(f\"âœ… PCA reduced {X_train_sel.shape[1]} features â†’ {X_train_pca.shape[1]} components\")\n\n# --- Step 7: Train Decision Tree on PCA-reduced data ---\ndt_pca = DecisionTreeClassifier(\n    random_state=29325, \n    class_weight='balanced',\n    max_depth=6,\n    min_samples_leaf=20\n)\ndt_pca.fit(X_train_pca, y_train)\n\n# --- Step 8: Evaluate the model ---\ny_pred_proba = dt_pca.predict_proba(X_test_pca)[:, 1]\ny_pred = dt_pca.predict(X_test_pca)\n\nprint(\"\\n=== Decision Tree (PCA 90% Variance) ===\")\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\nprint(classification_report(y_test, y_pred))\n\n# --- Optional: Show explained variance per component ---\nprint(\"\\nðŸ“Š Explained Variance Ratio per Component:\")\nfor i, var in enumerate(pca_reduced.explained_variance_ratio_, 1):\n    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%)\")\nprint(f\"\\nTotal Variance Explained: {pca_reduced.explained_variance_ratio_.sum():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report, make_scorer\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ========================================\n# HYPERPARAMETER TUNING - GRID SEARCH\n# ========================================\n\nprint(\"=\" * 60)\nprint(\"HYPERPARAMETER TUNING - GRID SEARCH\")\nprint(\"=\" * 60)\n\n# Use your best feature set (forward_features from your notebook)\nX_train_tuning = X_train[forward_features]\nX_test_tuning = X_test[forward_features]\n\n# Define parameter grid\nparam_grid = {\n    'max_depth': [4, 6, 8, 10, 12],\n    'min_samples_split': [10, 20, 30, 50],\n    'min_samples_leaf': [10, 20, 30, 50],\n    'class_weight': ['balanced', {0: 1, 1: 10}, {0: 1, 1: 20}],\n    'criterion': ['gini', 'entropy']\n}\n\n# Initialize Decision Tree\ndt_grid = DecisionTreeClassifier(random_state=29325)\n\n# ROC-AUC scorer\nroc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n\n# GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=dt_grid,\n    param_grid=param_grid,\n    scoring=roc_auc_scorer,\n    cv=3,  # 3-fold cross-validation\n    n_jobs=-1,\n    verbose=2\n)\n\nprint(\"\\nðŸ” Running Grid Search...\")\ngrid_search.fit(X_train_tuning, y_train)\n\nprint(\"\\nâœ… Best Parameters Found:\")\nprint(grid_search.best_params_)\nprint(f\"\\nâœ… Best Cross-Validation ROC-AUC: {grid_search.best_score_:.4f}\")\n\n# Evaluate on test set\nbest_model_grid = grid_search.best_estimator_\ny_pred_proba_grid = best_model_grid.predict_proba(X_test_tuning)[:, 1]\ny_pred_grid = best_model_grid.predict(X_test_tuning)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"GRID SEARCH - TEST SET PERFORMANCE\")\nprint(\"=\" * 60)\nprint(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_grid):.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_grid))\n\n\n# ========================================\n# HYPERPARAMETER TUNING - RANDOMIZED SEARCH\n# ========================================\n\nprint(\"\\n\\n\" + \"=\" * 60)\nprint(\"HYPERPARAMETER TUNING - RANDOMIZED SEARCH\")\nprint(\"=\" * 60)\n\n# Define parameter distributions (wider range)\nparam_distributions = {\n    'max_depth': [3, 4, 5, 6, 7, 8, 10, 12, 15, 20],\n    'min_samples_split': [5, 10, 15, 20, 30, 50, 100],\n    'min_samples_leaf': [5, 10, 15, 20, 30, 50, 100],\n    'class_weight': ['balanced', {0: 1, 1: 5}, {0: 1, 1: 10}, \n                     {0: 1, 1: 15}, {0: 1, 1: 20}],\n    'criterion': ['gini', 'entropy'],\n    'max_features': [None, 'sqrt', 'log2', 0.5, 0.7, 0.9]\n}\n\n# RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=DecisionTreeClassifier(random_state=29325),\n    param_distributions=param_distributions,\n    n_iter=50,  # Number of random combinations to try\n    scoring=roc_auc_scorer,\n    cv=3,\n    n_jobs=-1,\n    verbose=2,\n    random_state=29325\n)\n\nprint(\"\\nðŸ” Running Randomized Search (50 iterations)...\")\nrandom_search.fit(X_train_tuning, y_train)\n\nprint(\"\\nâœ… Best Parameters Found:\")\nprint(random_search.best_params_)\nprint(f\"\\nâœ… Best Cross-Validation ROC-AUC: {random_search.best_score_:.4f}\")\n\n# Evaluate on test set\nbest_model_random = random_search.best_estimator_\ny_pred_proba_random = best_model_random.predict_proba(X_test_tuning)[:, 1]\ny_pred_random = best_model_random.predict(X_test_tuning)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RANDOMIZED SEARCH - TEST SET PERFORMANCE\")\nprint(\"=\" * 60)\nprint(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_random):.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_random))\n\n\n# ========================================\n# FEATURE IMPORTANCE - TOP 10 FEATURES\n# ========================================\n\nprint(\"\\n\\n\" + \"=\" * 60)\nprint(\"FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\" * 60)\n\n# Get feature importances from best model\nfeature_importances = pd.DataFrame({\n    'Feature': forward_features,\n    'Importance': best_model_random.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\nðŸ“Š Top 10 Most Important Features:\")\nprint(feature_importances.head(10).to_string(index=False))\n\n# Plot feature importances\nplt.figure(figsize=(10, 6))\ntop_10 = feature_importances.head(10)\nplt.barh(top_10['Feature'], top_10['Importance'], color='steelblue')\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Top 10 Feature Importances (Decision Tree)', fontsize=14)\nplt.gca().invert_yaxis()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# ========================================\n# COMPARISON OF ALL APPROACHES\n# ========================================\n\nprint(\"\\n\\n\" + \"=\" * 60)\nprint(\"ðŸ“ˆ PERFORMANCE SUMMARY - ALL APPROACHES\")\nprint(\"=\" * 60)\n\n# Calculate baseline (from your notebook)\ndt_baseline = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',\n    max_depth=6,\n    min_samples_leaf=20\n)\ndt_baseline.fit(X_train_tuning, y_train)\ny_pred_proba_baseline = dt_baseline.predict_proba(X_test_tuning)[:, 1]\nbaseline_auc = roc_auc_score(y_test, y_pred_proba_baseline)\n\n# Create summary DataFrame\nresults_summary = pd.DataFrame({\n    'Approach': [\n        '1. Baseline (Default DT)',\n        '2. After Feature Selection',\n        '3. Grid Search Tuning',\n        '4. Randomized Search Tuning'\n    ],\n    'ROC-AUC': [\n        baseline_auc,\n        0.6007,  # From your forward selection result\n        roc_auc_score(y_test, y_pred_proba_grid),\n        roc_auc_score(y_test, y_pred_proba_random)\n    ]\n})\n\nresults_summary['Improvement'] = results_summary['ROC-AUC'] - baseline_auc\n\nprint(\"\\n\" + results_summary.to_string(index=False))\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\ncolors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']\nbars = plt.bar(results_summary['Approach'], results_summary['ROC-AUC'], \n               color=colors, alpha=0.7, edgecolor='black')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Random Classifier', alpha=0.5)\nplt.ylabel('ROC-AUC Score', fontsize=12)\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\nplt.xticks(rotation=15, ha='right')\nplt.ylim([0.4, max(results_summary['ROC-AUC']) + 0.05])\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Hyperparameter tuning complete!\")\nprint(f\"ðŸ† Best Model: {'Grid Search' if roc_auc_score(y_test, y_pred_proba_grid) > roc_auc_score(y_test, y_pred_proba_random) else 'Randomized Search'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}