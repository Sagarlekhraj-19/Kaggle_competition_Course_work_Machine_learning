# ğŸš— Enhanced Safe Driver Prediction Challenge

> **Machine Learning Project**: Predicting insurance claim probability using advanced classification techniques

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![Kaggle](https://img.shields.io/badge/Kaggle-Competition-20BEFF.svg)](https://www.kaggle.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Problem Statement](#-problem-statement)
- [Dataset](#-dataset)
- [Project Structure](#-project-structure)
- [Models Implemented](#-models-implemented)
- [Results](#-results)
- [Installation](#-installation)
- [Usage](#-usage)
- [Key Findings](#-key-findings)
- [Contributing](#-contributing)
- [Author](#-author)

---

## ğŸ¯ Overview

This project tackles the **Enhanced Safe Driver Prediction Challenge**, focusing on predicting the probability that an auto insurance policyholder will file a claim. Built on an improved version of the Porto Seguro dataset, this project emphasizes:

- âœ¨ **Smart Feature Engineering**
- âš–ï¸ **Handling Severely Imbalanced Data** (94.9% vs 5.1%)
- ğŸ¯ **Maximizing AUROC Performance**
- ğŸ”„ **Robust Cross-Validation**

### ğŸ† Competition Achievement

**Best Model**: CatBoost with Hyperparameter Tuning
- **Kaggle Public Score**: `0.64138` ğŸ¥‡
- **Training Time**: 12.5 hours
- **Key Success**: Perfect generalization (CV score matched Kaggle score)

---

## ğŸ’¡ Problem Statement

Insurance companies must assess risk to determine premiums and minimize financial losses. This project develops a machine learning classifier to:

- ğŸ“Š Predict claim probability based on policyholder and vehicle features
- ğŸ¯ Achieve high AUROC for effective risk discrimination
- ğŸ’° Enable personalized premium pricing
- ğŸš« Reduce fraudulent claims

---

## ğŸ“Š Dataset

### Statistics

| Metric | Value |
|--------|-------|
| **Training Samples** | 296,209 |
| **Test Samples** | 126,948 |
| **Features** | 67 |
| **Numeric Variables** | 37 |
| **Categorical Variables** | 30 |
| **Class Imbalance** | 18.5:1 |

### Feature Categories

```
ğŸ“ Individual Variables (ps_ind_*)
ğŸš™ Car-Related Variables (ps_car_*)
ğŸ—ºï¸ Regional Variables (ps_reg_*)
ğŸ§® Calculated Variables (ps_calc_*)
âš™ï¸ Engineered Features (feature1-8)
ğŸ¯ Target Variable (binary: 0/1)
```

### Data Quality Challenges

- âš ï¸ **Missing Data**: Up to 69% in some features
- âš–ï¸ **Severe Class Imbalance**: 94.9% non-claims
- ğŸ”— **High Correlation**: 21 variables flagged
- 0ï¸âƒ£ **Zero-Inflation**: Multiple variables

---

## ğŸ“ Project Structure

```
enhanced-safe-driver-prediction/
â”‚
â”œâ”€â”€ ğŸ““ kaggle.ipynb                 # Main training notebook
â”œâ”€â”€ ğŸ“„ kaggle_report.pdf           # Comprehensive project report
â”‚
â”œâ”€â”€ ğŸ“Š Data/
â”‚   â”œâ”€â”€ train1.csv                 # Training dataset
â”‚   â””â”€â”€ test.csv                   # Test dataset
â”‚
â”œâ”€â”€ ğŸ’¾ Models/
â”‚   â”œâ”€â”€ submission_CatBoost.csv    # Winner! ğŸ†
â”‚   â”œâ”€â”€ submission_RandomForest.csv
â”‚   â”œâ”€â”€ submission_AdaBoost.csv
â”‚   â”œâ”€â”€ submission_DecisionTree.csv
â”‚   â”œâ”€â”€ submission_KNN.csv
â”‚   â””â”€â”€ submission_NaiveBayes.csv
â”‚
â”œâ”€â”€ ğŸ“ˆ Visualizations/
â”‚   â””â”€â”€ model_training_comparison.png
â”‚
â””â”€â”€ ğŸ“– README.md                   # This file
```

---

## ğŸ¤– Models Implemented

### 1. Categorical Naive Bayes
- âš¡ **Training Time**: 1.86s
- ğŸ“Š **Train AUROC**: 0.6423
- ğŸ¯ **Kaggle Score**: Not submitted
- ğŸ’­ **Note**: Fast baseline with independence assumption

### 2. K-Nearest Neighbors (k=5)
- âš¡ **Training Time**: 4.21s
- ğŸ“Š **Train AUROC**: 0.9240 (Highest!)
- ğŸ¯ **Kaggle Score**: 0.50623 (Worst - Overfitting!)
- âš ï¸ **Warning**: Memorized training data

### 3. Decision Tree (depth=10)
- âš¡ **Training Time**: 12.50s
- ğŸ“Š **Train AUROC**: 0.6743
- ğŸ¯ **Kaggle Score**: 0.57333
- ğŸ“‹ **Nodes**: 1,023 | **Leaves**: 512

### 4. Random Forest (100 trees)
- âš¡ **Training Time**: 48.85s
- ğŸ“Š **Train AUROC**: 0.9116
- ğŸ¯ **Kaggle Score**: 0.59801
- âš ï¸ **Issue**: 34% performance drop (overfitting)

### 5. AdaBoost (100 estimators)
- âš¡ **Training Time**: 341.44s
- ğŸ“Š **Train AUROC**: 0.6438
- ğŸ¯ **Kaggle Score**: 0.63016 (3rd place)
- ğŸ’¡ **Strength**: Good with imbalanced data

### 6. CatBoost (Grid Search) ğŸ†
- âš¡ **Training Time**: 45,005s (12.5 hours)
- ğŸ“Š **Train AUROC**: 0.6383 (CV)
- ğŸ¯ **Kaggle Score**: 0.64138 (BEST!)
- ğŸ¨ **Parameters**: 243 combinations Ã— 3 folds = 729 fits
- âœ¨ **Key**: Perfect generalization (CV matched Kaggle)

#### Optimal Hyperparameters
```python
{
    'iterations': 500,
    'learning_rate': 0.03,
    'depth': 6,
    'l2_leaf_reg': 5,
    'border_count': 32,
    'class_weights': [1, 5]
}
```

---

## ğŸ“ˆ Results

### Final Kaggle Leaderboard

| Rank | Model | Kaggle Score | Training AUROC | Gap |
|------|-------|--------------|----------------|-----|
| ğŸ¥‡ 1 | **CatBoost** | **0.64138** | 0.6383 | **+0.5%** âœ… |
| ğŸ¥ˆ 2 | CatBoost v2 | 0.63825 | 0.6383 | Â±0.0% |
| ğŸ¥‰ 3 | AdaBoost | 0.63016 | 0.6438 | -2.1% |
| 4 | Decision Tree | 0.57333 | 0.6743 | -15.0% |
| 5 | Random Forest | 0.59801 | 0.9116 | **-34.4%** âš ï¸ |
| 6 | KNN | 0.50623 | 0.9240 | **-45.2%** ğŸš« |

### Performance Visualization

![Model Comparison](Visualizations/model_training_comparison.png)

### Top 10 Important Features (CatBoost)

| Rank | Feature | Importance | Category |
|------|---------|------------|----------|
| 1 | `ps_ind_03` | 9.3370 | Individual |
| 2 | `ps_car_13` | 7.1361 | Car |
| 3 | `ps_reg_01` | 4.9687 | Regional |
| 4 | `ps_ind_15` | 4.6452 | Individual |
| 5 | `ps_reg_02` | 3.6249 | Regional |
| 6 | `ps_ind_05_cat_0.0` | 3.5811 | Categorical |
| 7 | `ps_ind_17_bin` | 3.3785 | Binary |
| 8 | `ps_reg_03` | 3.1532 | Regional |
| 9 | `feature4` | 2.5495 | Engineered |
| 10 | `ps_car_14` | 2.4677 | Car |

---

## ğŸš€ Installation

### Prerequisites

```bash
Python 3.10+
Jupyter Notebook
```

### Required Libraries

```bash
pip install numpy pandas scikit-learn matplotlib seaborn
pip install catboost xgboost lightgbm
pip install jupyter notebook
```

### Clone Repository

```bash
git clone https://github.com/yourusername/safe-driver-prediction.git
cd safe-driver-prediction
```

---

## ğŸ’» Usage

### 1. Data Preparation

```python
# Load data
import pandas as pd
train = pd.read_csv('train1.csv')
test = pd.read_csv('test.csv')

# Check dimensions
print(f"Training: {train.shape}")
print(f"Testing: {test.shape}")
```

### 2. Run Training Pipeline

```bash
jupyter notebook kaggle.ipynb
```

### 3. Generate Predictions

All models generate submission files:
```
submission_CatBoost.csv      # Best model
submission_RandomForest.csv
submission_AdaBoost.csv
submission_DecisionTree.csv
submission_KNN.csv
```

### 4. Submit to Kaggle

```bash
kaggle competitions submit -c [competition-name] -f submission_CatBoost.csv -m "CatBoost submission"
```

---

## ğŸ”‘ Key Findings

### ğŸ¯ Critical Lessons

#### 1. **Training Scores Are Deceptive**
```
KNN: 0.924 training â†’ 0.506 Kaggle (-45% drop!)
CatBoost: 0.638 CV â†’ 0.641 Kaggle (+0.5% gain!)
```
**Lesson**: Never trust training metrics without proper cross-validation.

#### 2. **Conservative Parameters Win**
Initially criticized settings proved optimal:
- `l2_leaf_reg=5` (max regularization)
- `learning_rate=0.03` (slow learning)
- Prevented overfitting that destroyed Random Forest

#### 3. **Time Investment Pays Off**
- 12.5 hours training â†’ 1st place
- 729 model fits prevented overfitting
- Thoroughness beats speed in competitions

#### 4. **Class Imbalance Handling**
```python
class_weights = [1, 5]  # 5x weight for minority class
```
Essential for AUROC performance on imbalanced data.

---

## ğŸ§® Data Preprocessing Pipeline

```python
# 1. Handle Missing Values
cat_imputer = SimpleImputer(strategy='most_frequent')
num_imputer = SimpleImputer(strategy='mean')

# 2. Drop High-Missing Features
drop_cols = ['ps_car_03_cat', 'ps_car_05_cat']  # 69%, 45% missing

# 3. One-Hot Encoding
encoder = OneHotEncoder(handle_unknown='ignore')
X_encoded = encoder.fit_transform(X[cat_cols])

# 4. Feature Scaling (for KNN)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# 5. Final Dataset
X_final = pd.concat([X_encoded, X_numeric, X_binary], axis=1)
```

---

## ğŸ“Š Model Evaluation Metrics

### Primary Metric: AUROC
```python
from sklearn.metrics import roc_auc_score

auroc = roc_auc_score(y_true, y_pred_proba)
```

### Why AUROC?
- âœ… Handles class imbalance
- âœ… Threshold-independent
- âœ… Measures discrimination ability
- âŒ Accuracy misleading (94% by predicting all zeros!)

---

## ğŸ“ Technical Highlights

### Grid Search Configuration
```python
param_grid = {
    'iterations': [300, 500, 700],
    'learning_rate': [0.03, 0.05, 0.1],
    'depth': [4, 6, 8],
    'l2_leaf_reg': [1, 3, 5],
    'border_count': [32, 64, 128]
}

# 3Ã—3Ã—3Ã—3Ã—3 = 243 combinations
# 243 Ã— 3 folds = 729 model fits
```

### Cross-Validation Strategy
```python
GridSearchCV(
    estimator=catboost_model,
    param_grid=param_grid,
    cv=3,  # 3-fold CV
    scoring='roc_auc',
    n_jobs=-1
)
```

---

## ğŸ¤ Contributing

Contributions welcome! Please follow these steps:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open Pull Request

---

## ğŸ‘¨â€ğŸ’» Author

**Sagar Lekhraj**
- ğŸ“ ERP: 29325
- ğŸ« Institution: IBA Karachi
- ğŸ“§ Email: [your-email@example.com]
- ğŸ”— LinkedIn: [Your LinkedIn Profile]
- ğŸ’» GitHub: [@yourusername](https://github.com/yourusername)

**Course**: CSE 472 - Introduction to Machine Learning  
**Instructor**: Dr. Sajjad Haider, PhD  
**Department**: Computer Science

---

## ğŸ“š References

1. Porto Seguro Safe Driver Prediction Dataset
2. Scikit-learn Documentation
3. CatBoost Official Documentation
4. Kaggle Competition Guidelines

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- IBA Karachi Computer Science Department
- Dr. Sajjad Haider for course guidance
- Kaggle community for inspiration
- CatBoost team for excellent documentation

---

<div align="center">

### â­ If you found this project helpful, please star the repository! â­

**Made with â¤ï¸ and â˜• by Sagar Lekhraj**

</div>

---

## ğŸ“… Project Timeline

```
Week 1: Data Exploration & EDA
Week 2: Preprocessing & Feature Engineering
Week 3: Baseline Models (Naive Bayes, KNN, Decision Tree)
Week 4: Ensemble Methods (Random Forest, AdaBoost)
Week 5: CatBoost Hyperparameter Tuning (12.5 hours!)
Week 6: Final Submission & Report
```

---

## ğŸ”® Future Work

- [ ] Implement SMOTE for better class balance
- [ ] Try XGBoost and LightGBM
- [ ] Deep learning approaches (Neural Networks)
- [ ] Ensemble stacking of top models
- [ ] Feature selection optimization
- [ ] Advanced feature engineering
- [ ] Bayesian optimization for hyperparameters

---

**Last Updated**: November 2024
